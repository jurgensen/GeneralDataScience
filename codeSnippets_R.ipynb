{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "install.packages(\"stm\", repos=\"http://cran.cnr.berkeley.edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preferred packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(sqldf)\n",
    "library(tm)\n",
    "library(h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read and write base csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- read.csv(\"train.csv\")\n",
    "write.csv(train, \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic exploration of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(10)\n",
    "assign <- runif(nrow(train_use), 0, 1)\n",
    "trn <- train_use[assign > 0.2, ]\n",
    "validate <- train_use[assign <= 0.2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z-score normalize both training and testing sets with same mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu <- lapply(trn[ , 2:ncol(trn)], mean)\n",
    "sigma <- lapply(trn[ , 2:ncol(trn)], sd)\n",
    "\n",
    "for (x in 2:ncol(train)){\n",
    "    m <- mu[(x-1)]\n",
    "    s <- sigma[(x-1)]\n",
    "    train[ , x] <- (train[ , x]-m[[1]])/s[[1]]\n",
    "}\n",
    "\n",
    "for (x in 2:ncol(train)){\n",
    "    m <- mu[(x-1)]\n",
    "    s <- sigma[(x-1)]\n",
    "    test[ , x] <- (test[ , x]-m[[1]])/s[[1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim(train)\n",
    "colnames(train)\n",
    "summary(train)\n",
    "head(train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mean, medians, etc. while removing NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means <- lapply(test[ ,65:ncol(test)], function(x) mean(x, na.rm=T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-test to test for statistical significance between population means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.test(train[train$author == \"MWS\", ]$a, train[train$author != \"MWS\", ]$a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert multiple logical features to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[ , reformat] <- lapply(train[ , reformat], function(x) ifelse(x==T, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Shaping and Manipulation\n",
    "column removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- train %>% select(-X, id) # with dplyr\n",
    "train <- train[ , 3:ncol(train)] # without dplyr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select only a subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stm <- train[ , c(\"id\", \"author\", \"text\")] # without dplyr\n",
    "train_stm <- train %>% select(id, author, text) # with dplyr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert factor level to a single binary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train$is_eap <- ifelse(train$author == \"EAP\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a series of column/row labels varying only by number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl <- seq(1, 61, by=1)\n",
    "lbl <- as.character(lbl)\n",
    "lbl <- sub(\"^\", \"topic\", lbl)\n",
    "cnames <- c(\"id\", lbl)\n",
    "colnames(train_processed) <- cnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "filter subset of filters based on threshold of correlation with any of several y features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations <- cor(train[, c(22:24, 398:647)]) # some of the correlations may be NA if the sd == 0\n",
    "correlations[is.na(correlations)] <- 0\n",
    "\n",
    "eap_corr <- correlations[1,]\n",
    "hpl_corr <- correlations[2,]\n",
    "mws_corr <- correlations[3,]\n",
    "\n",
    "eap_corr <- eap_corr[4:length(eap_corr)]\n",
    "hpl_corr <- hpl_corr[4:length(hpl_corr)]\n",
    "mws_corr <- mws_corr[4:length(mws_corr)]\n",
    "\n",
    "use1 <- names(eap_corr[eap_corr >= 0.02 | eap_corr <= -.02])\n",
    "use2 <- names(hpl_corr[hpl_corr >= 0.02 | hpl_corr <= -.02])\n",
    "use3 <- names(mws_corr[mws_corr >= 0.02 | mws_corr <= -.02])\n",
    "\n",
    "use <- c(use1, use2, use3)\n",
    "use <- unique(use) # these are the feature names to keep\n",
    "keep <- colnames(train[ , 1:397])\n",
    "train <- train[ , c(keep, use)] # final data frame doesn't have the features that didn't pass the correlation threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "Correlograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrplot(cor(train[ , c(\"is_eap\", \"is_hpl\", \"is_mws\", \"word_count\", \"n_comma\", \"n_commaSemiColon\", \"n_colons\")]), \n",
    "        addCoef.col = \"black\", number.cex = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text with Base\n",
    "base text and regular expression processing of single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test$lower <- tolower(test$text)\n",
    "test$all_char <- nchar(test$lower)\n",
    "train$AN_char <- nchar(gsub(\"[^a-zA-Z0-9]\",\"\", train$lower))\n",
    "train$percent_ANchar <- train$AN_char / train$all_char\n",
    "train$a <- nchar(gsub(\"[^a]\", \"\", train$lower))\n",
    "train$vowels <- nchar(gsub(\"[^aeiou]\", \"\", train$lower))\n",
    "train$percent_vowels <- train$vowels / train$AN_char\n",
    "train$bare <- gsub(\"[^a-z ]\", \"\", train$lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base and regex get word count of single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp <- train$bare\n",
    "temp <- gsub(\"\\\\b[a-z]+\\\\b\", \"a\", temp)\n",
    "temp <- gsub(\" \", \"\", temp)\n",
    "train$word_count <- nchar(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base, regex to create single binary column from multiple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3rd person male (he, him, his)\n",
    "temp <- gsub(\"\\\\bhe\\\\b\", \"A\", train$bare)\n",
    "temp <- gsub(\"[^A]\", \"\", temp)\n",
    "temp_sum <- nchar(temp)\n",
    "\n",
    "temp <- gsub(\"\\\\bhim\\\\b\", \"A\", train$bare)\n",
    "temp <- gsub(\"[^A]\", \"\", temp)\n",
    "temp <- nchar(temp)\n",
    "temp_sum <- temp_sum + temp\n",
    "\n",
    "temp <- gsub(\"\\\\bhis\\\\b\", \"A\", train$bare)\n",
    "temp <- gsub(\"[^A]\", \"\", temp)\n",
    "temp <- nchar(temp)\n",
    "temp_sum <- temp_sum + temp\n",
    "\n",
    "train$has_he <- ifelse(temp_sum > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base, regex create single binary column or numeric column from list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepositions <- c(\"on\", \"at\", \"to\", \"by\")\n",
    "temp <- test$bare\n",
    "for (x in prepositions){\n",
    "    temp <- gsub(paste0(\"\\\\b\", x, \"\\\\b\"), \"A\", temp)\n",
    "}\n",
    "temp <- gsub(\"[^A]\", \"\", temp)\n",
    "test$number_prepositions <- nchar(temp)\n",
    "test$has_preposition <- ifelse(test$number_prepositions > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base, regex create multiple binary columns from list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first <- c(\"pa\", \"ju\", \"af\", \"da\", \"on\", \"gu\")\n",
    "for (x in first){\n",
    "    lbl = paste0(\"first_\", x)\n",
    "    train[ ,lbl] <- grepl(paste0(\"\\\\b\", x, \"\\\\w\"), train$bare)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text with TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(tm)\n",
    "library(snowballC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create corpus from multiple text files (or a text file)\n",
    "\n",
    "<ul>\n",
    "<li>https://stackoverflow.com/questions/7927367/r-text-file-and-text-mining-how-to-load-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source <- DirSource(\"more_text/\") #input path for documents\n",
    "AuthorsCorpus <- Corpus(source, readerControl=list(reader=readPlain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create corpus of bi-grams from a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(RWeka)\n",
    "\n",
    "options(mc.cores=4)\n",
    "twogramTokenizer <- function(x) {\n",
    "    NGramTokenizer(x, Weka_control(min=2, max=2))\n",
    "}\n",
    "\n",
    "poe_2dtm <- DocumentTermMatrix(poe_corpus, control=list(tokenize=twogramTokenizer))\n",
    "poe_2dtm\n",
    "poe_2dtm <- removeSparseTerms(poe_2dtm, 0.999)\n",
    "poe_2dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create and process corpus from data frame column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp_eap <- Corpus(VectorSource(train[train$author == \"EAP\",]$bare))\n",
    "\n",
    "corp_eap <- tm_map(corp_eap, stripWhitespace)\n",
    "dtm_eap <- DocumentTermMatrix(corp_eap)\n",
    "dtm_eap\n",
    "dim(dtm_eap)\n",
    "eap_words <- colnames(dtm_eap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem words of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp_eap_stem <- tm_map(corp_eap, stemDocument, language = \"english\")\n",
    "dtm_eap_stem <- DocumentTermMatrix(corp_eap_stem)\n",
    "dtm_eap_stem\n",
    "dtm_eap_stem <- removeSparseTerms(dtm_eap_stem, 0.99)\n",
    "dtm_eap_stem\n",
    "eap_stems <- colnames(dtm_eap_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create list of words that are unique to one of multiple corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_eap_stems <- unique(c(mws_stems, hpl_stems))\n",
    "not_mws_stems <- unique(c(eap_stems, hpl_stems))\n",
    "not_hpl_stems <- unique(c(eap_stems, mws_stems))\n",
    "\n",
    "eap_only <- eap_stems[! (eap_stems %in% not_eap_stems)]\n",
    "mws_only <- mws_stems[!(mws_stems %in% not_mws_stems)]\n",
    "hpl_only <- hpl_stems[! (hpl_stems %in% not_hpl_stems)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find words of interest in a general corpus, convert the features to binary from logical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_of_interest <- c(eap_only, mws_only, hpl_only)\n",
    "for (word in words_of_interest){\n",
    "    lbl <- paste0(\"has_\", word)\n",
    "    train2[ , lbl] <- grepl(word, train2$bare)\n",
    "}\n",
    "train2[18:245] <- lapply(train2[18:245], function(x) ifelse(x == TRUE, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with stm\n",
    "\n",
    "### structural topic models with stm\n",
    "\n",
    "<ul>\n",
    "<li>https://www.rdocumentation.org/packages/stm/versions/1.3.0</li>\n",
    "<li>https://www.rdocumentation.org/packages/stm/versions/1.3.0/topics/fitNewDocuments</li>\n",
    "<li>https://www.rdocumentation.org/packages/stm/versions/1.3.0</li>\n",
    "<li>https://www.rdocumentation.org/packages/stm/versions/1.3.0/topics/selectModel</li>\n",
    "<li>https://www.rdocumentation.org/packages/stm/versions/1.3.0/topics/searchK</li>\n",
    "<li>https://www.rdocumentation.org/packages/stm/versions/1.3.0/topics/toLDAvis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(stm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build corpus using stm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_processed <- textProcessor(train_stm$text, metadata = train[ , c(\"id\", \"author\")], lowercase = TRUE,\n",
    "                                  removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE,\n",
    "                                  stem = TRUE, wordLengths = c(3, Inf), sparselevel = 1,\n",
    "                                  language = \"en\", verbose = TRUE, onlycharacter = FALSE)\n",
    "train_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta <- train_processed$meta\n",
    "vocab <- train_processed$vocab\n",
    "docs <- train_processed$documents\n",
    "train_out <- prepDocuments(docs, vocab, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1002)\n",
    "train_model <- stm(train_out$documents, train_out$vocab, 0, init.type=\"Spectral\", \n",
    "                   data=train_out$meta$author) #prevalence=~treatment + s(pid_rep)  \n",
    "# eventually used to create features to predict \"author\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot comparissons for 2 of the topics at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.STM(train_model, type=\"perspectives\", topics=c(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot distribution of MAP estimates of document-topic proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.STM(train_model, type=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot topic correlation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(topicCorr(train_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply the model from the training set to the test set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp <- textProcessor(test_stm$text, metadata = test_stm)\n",
    "newdocs <- alignCorpus(new = temp, old.vocab = train_model$vocab)\n",
    "newdocs_fit <- fitNewDocuments(model = train_model, documents = newdocs$documents, \n",
    "                               newData=newdocs$meta, origData=train_out$meta) #didn't have 'author' in meta here, but accepted anyway\n",
    "#, prevalence=~treatment + s(pid_rep),prevalencePrior=\"Covariate\"))\n",
    "\n",
    "dim(newdocs_fit$theta) # thetas are measurements for each of the topics \n",
    "# will need to impute column means (or other values) for any docs with no topic info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figure out which documents in test set didn't have any topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_modeled <- newdocs$meta$id\n",
    "all_docs <- test_stm$id\n",
    "setdiff(all_docs, docs_modeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add topic model theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_vect <- as.character(newdocs$meta$id)\n",
    "test_processed <- cbind(id_vect, newdocs_fit$theta)\n",
    "cnames <- c(\"id\", lbl) #use 'lbl', a previously created list of label names for the topics\n",
    "colnames(test_processed) <- cnames\n",
    "test_processed <- data.frame(test_processed)\n",
    "\n",
    "test <- left_join(test, test_processed, by = \"id\") # add the topic model theta values as features to test frame\n",
    "test[ ,65:ncol(test)] <- lapply(test[ ,65:ncol(test)], function(x) as.numeric(as.character(x))) # are added as factors\n",
    "    # convert to character, then numeric (just numeric produces wrong numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impute column means for documents that didn't have topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(i in 65:ncol(test)){\n",
    "  test[is.na(test[,i]), i] <- mean(test[,i], na.rm = TRUE)\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with topicmodels\n",
    "\n",
    "create a multi-document corpus from multiple text files in a folder\n",
    "\n",
    "<ul>\n",
    "<li>https://rstudio-pubs-static.s3.amazonaws.com/163802_0f005a14bcfb4c4b8ee17ac8a8e6c3e9.html</li>\n",
    "<li>https://stackoverflow.com/questions/7927367/r-text-file-and-text-mining-how-to-load-data</li>\n",
    "<li>http://www.mjdenny.com/Text_Processing_In_R.html</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(tm)\n",
    "library(topicmodels)\n",
    "\n",
    "source <- DirSource(\"more_text/\") #input path for documents\n",
    "AuthorsCorpus <- Corpus(source, readerControl=list(reader=readPlain))  \n",
    "summary(AuthorsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AuthorsCorpus <- tm_map(AuthorsCorpus, content_transformer(tolower))\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, removeWords, stopwords(kind=\"en\"))\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, removeWords, c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \n",
    "                                     \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \n",
    "                                     \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")) #remove single letters\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, removeWords, c(\"lovecraft\", \"poe\", \"shelley\")) #remove their names, just in case\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, removePunctuation)\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, removeNumbers)\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, stripWhitespace)\n",
    "\n",
    "inspect(AuthorsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examine the metadata of one of the documents in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta(AuthorsCorpus[[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add and populate a metadata field for the documents in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta(AuthorsCorpus[[1]], \"category\") <- \"HPL\"\n",
    "meta(AuthorsCorpus[[2]], \"category\") <- \"EAP\"\n",
    "meta(AuthorsCorpus[[3]], \"category\") <- \"MWS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem the documents, create document-term matrix using term frequency\n",
    "\n",
    "create a dictionary corpus and use it to map the original words to the stemmed words  (doesn't work--- solve?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictCorpus = AuthorsCorpus\n",
    "AuthorsCorpus <- tm_map(AuthorsCorpus, stemDocument)\n",
    "#AuthorsCorpus <- tm_map(AuthorsCorpus, stemCompletion, dictionary=dictCorpus)  \n",
    "dtm <- DocumentTermMatrix(AuthorsCorpus, control = list(minWordLength = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view terms from the dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Terms(dtm)[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dtm using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm2 = DocumentTermMatrix(AuthorsCorpus, control = list(weighting = weightTfIdf, minWordLength = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list the frequent terms in the corpus\n",
    "<li>https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findFreqTerms(dtm, lowfreq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove sparse words from dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm <- removeSparseTerms(dtm, 0.999)\n",
    "dtm2 <- removeSparseTerms(dtm2, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>train models using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k <- 30\n",
    "SEED <- 1234\n",
    "my_TM <- list(VEM = LDA(dtm, k = k, control = list(seed = SEED)),\n",
    "              VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),\n",
    "              Gibbs = LDA(dtm, k = k, method = \"Gibbs\", control = list(seed = SEED, burnin = 1000, \n",
    "                                                          thin = 100, iter = 1000)), \n",
    "              CTM = CTM(dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), \n",
    "                                                    em = list(tol = 10^-3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the topics for one of the methods used for modeling, and the top terms for it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Topic = topics(my_TM[[\"VEM\"]], 1)\n",
    "Topic\n",
    "\n",
    "Terms = terms(my_TM[[\"VEM\"]], 5) #top 5 terms for each topic in LDA\n",
    "Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the sets of terms for each topic in a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (x in 1:ncol(Terms)){\n",
    "    print(paste(Terms[1:5, x], collapse = \", \"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the most frequent terms used in the topics of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_topics = topics(my_TM[[\"Gibbs\"]])\n",
    "most_frequent = which.max(tabulate(my_topics))\n",
    "terms(my_TM[[\"Gibbs\"]], 10)[, most_frequent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - h2o\n",
    "\n",
    "initialize h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2o.init(nthreads=5, max_mem_size = \"4G\")    \n",
    "## specify the memory size for the H2O cloud; default nthreads (-1) is maximum number of CPUS\n",
    "\n",
    "h2o.removeAll() # Clean slate - just in case the cluster was already running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- h2o.importFile(\"train_to_use_121417.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf1 <- h2o.randomForest(training_frame = train, \n",
    "                        y=1, ntrees=20,    ## use a maximum of 20 trees to start.. (default 50)\n",
    "                        max_depth=20, stopping_rounds=2, \n",
    "                        ## stop fitting new trees when 2-tree avg w/in 0.001 (default) of prior two 2-tree avgs\n",
    "                        seed=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get results from categorical prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions <- h2o.predict(object = rf1 ,newdata = test)\n",
    "cat(\"Overall Accuracy: \", nrow(test[test$author == predictions$predict, ])/nrow(test), \n",
    "       \"\\nEAP prec: \" , nrow(predictions[predictions$predict == \"EAP\" & test$author == \"EAP\", ])/nrow(predictions[predictions$predict == \"EAP\", ]), \n",
    "        \", EAP recall: \", nrow(test[test$author == \"EAP\" & predictions$predict == \"EAP\", ])/nrow(test[test$author == \"EAP\", ]), \n",
    "        \", EAP acc:\", (nrow(test[test$author == \"EAP\" & predictions$predict == \"EAP\", ]) + \n",
    "                       nrow(test[test$author != \"EAP\" & predictions$predict != \"EAP\", ])) / nrow(test),\n",
    "        \"\\nHPL prec: \", nrow(predictions[predictions$predict == \"HPL\" & test$author == \"HPL\", ])/nrow(predictions[predictions$predict == \"HPL\", ]), \n",
    "        \", HPL recall: \", nrow(test[test$author == \"HPL\" & predictions$predict == \"HPL\", ])/nrow(test[test$author == \"HPL\", ]), \n",
    "        \", HPL acc:\", (nrow(test[test$author == \"HPL\" & predictions$predict == \"HPL\", ]) + \n",
    "                       nrow(test[test$author != \"HPL\" & predictions$predict != \"HPL\", ])) / nrow(test),\n",
    "        \"\\nMWS prec:\" , nrow(predictions[predictions$predict == \"MWS\" & test$author == \"MWS\", ])/nrow(predictions[predictions$predict == \"MWS\", ]), \n",
    "        \", MWS recall: \", nrow(test[test$author == \"MWS\" & predictions$predict == \"MWS\", ])/nrow(test[test$author == \"MWS\", ]), \n",
    "        \", MWS acc: \", (nrow(test[test$author == \"MWS\" & predictions$predict == \"MWS\", ]) + \n",
    "                       nrow(test[test$author != \"MWS\" & predictions$predict != \"MWS\", ])) / nrow(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get model desctiption and variable importance info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf1\n",
    "h2o.varimp(h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export h2o data frame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2o.exportFile(submission, \"submission_121417.csv\", force = TRUE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
