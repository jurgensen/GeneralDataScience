{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resources\n",
    "\n",
    "<ul>\n",
    "<li>https://chrisalbon.com/</li>\n",
    "<li>https://stackoverflow.com/questions/44173624/how-to-nltk-word-tokenize-to-a-pandas-dataframe-for-twitter-data</li>\n",
    "<li>https://www.dataquest.io/blog/natural-language-processing-with-python/</li>\n",
    "<li>https://stackoverflow.com/questions/34784004/python-text-processing-nltk-and-pandas</li>\n",
    "<li>https://stackoverflow.com/questions/37443138/python-stemming-with-pandas-dataframe</li>\n",
    "<li>https://stackoverflow.com/questions/18936957/count-distinct-words-from-a-pandas-data-frame</li>\n",
    "<li>https://pandas.pydata.org/pandas-docs/stable/indexing.html</li>\n",
    "<li>https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf</li>\n",
    "<li>http://www.webpages.uidaho.edu/~stevel/504/Pandas%20DataFrame%20Notes.pdf</li>\n",
    "<li>https://chrisalbon.com/python/pandas_convert_categorical_to_dummies.html</li>\n",
    "<li>http://mathesaurus.sourceforge.net/r-numpy.html</li>\n",
    "<li>http://www.datasciencefree.com/cheatsheets.html</li>\n",
    "<li>http://www.data-analysis-in-python.org/python_for_r.html</li>\n",
    "<li>https://www.dataquest.io/blog/python-vs-r/</li>\n",
    "<li>https://rstudio.github.io/reticulate/articles/arrays.html</li>\n",
    "<li>https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## General\n",
    "\n",
    "preferred packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read a CSV into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_121117.csv\", sep=',', error_bad_lines=False, encoding='latin1')#, encoding='utf-8') #header=None, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a data frame as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv(\"test_121317.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.drop('Unnamed: 0', axis=1) \n",
    "# column is named \"Unnamed: 0'\n",
    "# axis=1, here, means columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.columns.get_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see values for a specific column for a few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[1:5][\"tokenized_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get summary information for a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[\"word1_preposition\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.query('has_altogeth != 0')[0:3] #using column/feature \"has_altogeth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mean of numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"all_char\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Shaping and Manipulation\n",
    "\n",
    "create a vector/list from a data frame column, operated upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_word = test[\"bare\"].apply(lambda x: re.search(r\"^\\s*(\\w*)\\b\", x).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data set into train and validation set using random sampling from uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1002)\n",
    "train[\"assign\"] = np.random.uniform(low=0.0, high=1.0, size=train.shape[0])\n",
    "train_use = train[lambda x: x[\"assign\"] > 0.2]\n",
    "validate_use = train[lambda x: x[\"assign\"] <= 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing with re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search for a pattern and return the string/word found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.search(r\"^\\w*\\b\", test[\"bare\"][0]).group(0) \n",
    "# if use grouping, 0 returns entire hit, and 1+ return individual pattern groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search for a pattern in all the elements of a column and return the grouping found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_word = test[\"bare\"].apply(lambda x: re.search(r\"^\\s*(\\w*)\\b\", x).group(1)) # first word is a vector of words\n",
    "# if use grouping, 0 returns entire hit, and 1+ return individual pattern groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a feature counting the number members of a list of words contained in each string in a vector/column of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = first_word #create a copy of the vector/list (or column)\n",
    "for prep in prepositions: # use list of words 'prepositions'\n",
    "    term = \"\\\\b\"+prep+\"\\\\b\"\n",
    "    temp = temp.apply(lambda x: re.sub(term, \"A\", x))\n",
    "    \n",
    "temp = temp.apply(lambda x: re.sub(\"[^A]\", \"\", x))\n",
    "test[\"word1_preposition\"] = temp.apply(lambda x: len(x)) #if the column only contained single words, this will automatically be binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing with NLTK\n",
    "<ul>\n",
    "<li>https://stackoverflow.com/questions/37443138/python-stemming-with-pandas-dataframe\n",
    "<li>https://stackoverflow.com/questions/18936957/count-distinct-words-from-a-pandas-data-frame\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize and stem text in a data frame column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "train['tokenized_text'] = train['bare'].apply(word_tokenize)\n",
    "train[\"stemmed\"] = train[\"tokenized_text\"].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "train[\"stemmed\"] = train[\"stemmed\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create binary, dummy variable columns for the presence of specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_of_interest = ['account', 'altogeth', 'attent', 'between', 'case'] #these are stems\n",
    "for word in words_of_interest[1:2]:\n",
    "    lbl = \"has_\" + word\n",
    "    temp = train[\"stemmed\"].apply(lambda x: re.search(\"\\\\b\"+word+\"\\\\b\", x))\n",
    "    train[lbl] = temp.apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with h2o\n",
    "\n",
    "<ul>\n",
    "<li>http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html</li>\n",
    "<li>https://github.com/h2oai/h2o-tutorials</li>\n",
    "<li>http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.html</li>\n",
    "<li>https://h2o-release.s3.amazonaws.com/h2o/rel-slater/9/docs-website/h2o-py/docs/frame.html</li>\n",
    "<li>http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/sortcolumn.html</li>\n",
    "<li>http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/replacing-values.html</li>\n",
    "</ul>\n",
    "\n",
    "initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2o.init(nthreads=5, max_mem_size = \"4G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "get cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2o.cluster().show_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = h2o.upload_file(\"train_use_121217.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view columns; drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.columns\n",
    "train = train.drop(\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the type of a column; member of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"word_count\"].isnumeric()[0]\n",
    "type(mu[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find all numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerics = []\n",
    "for x in range(0, len(cnames)):\n",
    "    if cnames[x] in [\"is_eap\", \"is_hpl\", \"is_mws\"]: #these were numeric, but I didn't want to include \n",
    "        pass\n",
    "    elif temp[x] == True: \n",
    "        numerics.append(cnames[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mean and sd for all of the numeric columns; z-score normalize using mean and sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = []\n",
    "sigma = []\n",
    "for cname in numerics:\n",
    "    mu.append(train[cname].mean())\n",
    "    sigma.append(train[cname].sd())\n",
    "    \n",
    "for x in range(0, len(numerics)):\n",
    "    train[numerics[x]] = (train[numerics[x]] - mu[x][0])/sigma[x][0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf2 = h2o.estimators.random_forest.H2ORandomForestEstimator(ntrees = 50, max_depth = 20, seed = 1002)\n",
    "rf2.train(training_frame = train, y = \"author\", ignored_columns = [\"id\", \"is_eap\", \"is_hpl\", \"is_mws\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get predictions from trained model and overall accuracy of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf2_predictions = rf2.predict(test)\n",
    "rf2_results = test_authors == rf2_predictions[\"predict\"]\n",
    "len(rf2_results[rf2_results == 1])/len(test_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new frame from subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = submit[[0,288, 289, 290]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export h2o frame as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2o.export_file(results, \"submission_121317.csv\", force = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
